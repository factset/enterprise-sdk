/*
 * Events and Transcripts API
 * <p>The Calendar Events API provides access to FactSetâ€™s Event Calendar data alongside business logic that allows users to replicate views and functionality consistent with the experience provided by the Workstation. This API provides the ability to pull Event Calendar data based on specific filters.</p> <p>Events Audio API provides access to historical as well as the latest audio recordings of various company events covered by FactSet. The events include, but are not limited to: earnings calls, conferences, and investor days. This API also provides relevant metadata such as timestamps and identifiers around each audio file.</p> <p>The  Near Real-time Transcripts API enables access to Near Real-time Transcripts provided by CallStreet to time-sensitive clients. This API also provides the relevant speaker metadata along with their confidence scores. This data caters to quant clients interested in building machine learning models. Clients can leverage this API to perform sentiment analysis through natural language processing or machine learning. It can also be used to complement analysis using FactSet's transcripts service.</p> <p>Transcripts API provides conference call transcripts for companies' publicly held conference calls and a wealth of information regarding upcoming corporate events, such as conference call date and time, phone number and password, type of conference call, and important company investor relations contact information.</p> 
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: api@factset.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package com.factset.sdk.EventsandTranscripts.models;

import java.util.Objects;
import java.util.Arrays;
import java.util.Map;
import java.util.HashMap;
import com.factset.sdk.EventsandTranscripts.models.DocumentResult;
import com.factset.sdk.EventsandTranscripts.models.InvalidIdErrorData;
import com.factset.sdk.EventsandTranscripts.models.TranscriptsByIdsResponse;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonSubTypes;
import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.annotation.JsonTypeName;
import com.fasterxml.jackson.annotation.JsonValue;
import io.swagger.annotations.ApiModel;
import io.swagger.annotations.ApiModelProperty;
import java.time.LocalDate;
import java.time.OffsetDateTime;
import org.openapitools.jackson.nullable.JsonNullable;
import com.fasterxml.jackson.annotation.JsonIgnore;
import org.openapitools.jackson.nullable.JsonNullable;
import java.util.NoSuchElementException;
import java.io.Serializable;
import com.fasterxml.jackson.annotation.JsonPropertyOrder;
import com.factset.sdk.EventsandTranscripts.JSON;

import com.fasterxml.jackson.core.type.TypeReference;

import jakarta.ws.rs.core.GenericType;
import jakarta.ws.rs.core.Response;
import java.io.IOException;
import java.util.logging.Level;
import java.util.logging.Logger;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.math.BigDecimal;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.JsonToken;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationContext;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.MapperFeature;
import com.fasterxml.jackson.databind.SerializerProvider;
import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
import com.fasterxml.jackson.databind.annotation.JsonSerialize;
import com.fasterxml.jackson.databind.deser.std.StdDeserializer;
import com.fasterxml.jackson.databind.ser.std.StdSerializer;
import com.factset.sdk.EventsandTranscripts.JSON;

@jakarta.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen")

@JsonDeserialize(using = TranscriptsResponseData.TranscriptsResponseDataDeserializer.class)
@JsonSerialize(using = TranscriptsResponseData.TranscriptsResponseDataSerializer.class)
public class TranscriptsResponseData extends AbstractOpenApiSchema implements Serializable {
    private static final Logger log = Logger.getLogger(TranscriptsResponseData.class.getName());

    public static class TranscriptsResponseDataSerializer extends StdSerializer<TranscriptsResponseData> {
        public TranscriptsResponseDataSerializer(Class<TranscriptsResponseData> t) {
            super(t);
        }

        public TranscriptsResponseDataSerializer() {
            this(null);
        }

        @Override
        public void serialize(TranscriptsResponseData value, JsonGenerator jgen, SerializerProvider provider) throws IOException, JsonProcessingException {
            jgen.writeObject(value.getActualInstance());
        }
    }

    public static class TranscriptsResponseDataDeserializer extends StdDeserializer<TranscriptsResponseData> {
        public TranscriptsResponseDataDeserializer() {
            this(TranscriptsResponseData.class);
        }

        public TranscriptsResponseDataDeserializer(Class<?> vc) {
            super(vc);
        }

        @Override
        public TranscriptsResponseData deserialize(JsonParser jp, DeserializationContext ctxt) throws IOException, JsonProcessingException {
            JsonNode tree = jp.readValueAsTree();
            Object deserialized = null;
            TranscriptsResponseData newTranscriptsResponseData = new TranscriptsResponseData();
            Map<String,Object> result2 = tree.traverse(jp.getCodec()).readValueAs(new TypeReference<Map<String, Object>>() {});
            String discriminatorValue = (String)result2.get("transcriptResponseType");
            switch (discriminatorValue) {
                case "DocumentResult":
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(DocumentResult.class);
                    newTranscriptsResponseData.setActualInstance(deserialized);
                    return newTranscriptsResponseData;
                case "TranscriptsByIdsResponse":
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(TranscriptsByIdsResponse.class);
                    newTranscriptsResponseData.setActualInstance(deserialized);
                    return newTranscriptsResponseData;
                case "documentResult":
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(DocumentResult.class);
                    newTranscriptsResponseData.setActualInstance(deserialized);
                    return newTranscriptsResponseData;
                case "transcriptById":
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(TranscriptsByIdsResponse.class);
                    newTranscriptsResponseData.setActualInstance(deserialized);
                    return newTranscriptsResponseData;
                default:
                    log.log(Level.WARNING, String.format("Failed to lookup discriminator value `%s` for TranscriptsResponseData. Possible values: DocumentResult TranscriptsByIdsResponse documentResult transcriptById", discriminatorValue));
            }

            boolean typeCoercion = ctxt.isEnabled(MapperFeature.ALLOW_COERCION_OF_SCALARS);
            int match = 0;
            JsonToken token = tree.traverse(jp.getCodec()).nextToken();
            // deserialize TranscriptsByIdsResponse
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (TranscriptsByIdsResponse.class.equals(Integer.class) || TranscriptsByIdsResponse.class.equals(Long.class) || TranscriptsByIdsResponse.class.equals(Float.class) || TranscriptsByIdsResponse.class.equals(Double.class) || TranscriptsByIdsResponse.class.equals(Boolean.class) || TranscriptsByIdsResponse.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((TranscriptsByIdsResponse.class.equals(Integer.class) || TranscriptsByIdsResponse.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((TranscriptsByIdsResponse.class.equals(Float.class) || TranscriptsByIdsResponse.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (TranscriptsByIdsResponse.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (TranscriptsByIdsResponse.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(new TypeReference<TranscriptsByIdsResponse>() { });
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'TranscriptsByIdsResponse'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'TranscriptsByIdsResponse'", e);
            }

            // deserialize DocumentResult
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (DocumentResult.class.equals(Integer.class) || DocumentResult.class.equals(Long.class) || DocumentResult.class.equals(Float.class) || DocumentResult.class.equals(Double.class) || DocumentResult.class.equals(Boolean.class) || DocumentResult.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((DocumentResult.class.equals(Integer.class) || DocumentResult.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((DocumentResult.class.equals(Float.class) || DocumentResult.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (DocumentResult.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (DocumentResult.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(new TypeReference<DocumentResult>() { });
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'DocumentResult'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'DocumentResult'", e);
            }

            if (match == 1) {
                TranscriptsResponseData ret = new TranscriptsResponseData();
                ret.setActualInstance(deserialized);
                return ret;
            }
            throw new IOException(String.format("Failed deserialization for TranscriptsResponseData: %d classes match result, expected 1", match));
        }

        /**
         * Handle deserialization of the 'null' value.
         */
        @Override
        public TranscriptsResponseData getNullValue(DeserializationContext ctxt) throws JsonMappingException {
            throw new JsonMappingException(ctxt.getParser(), "TranscriptsResponseData cannot be null");
        }
    }

    // store a list of schema names defined in oneOf
    public static final Map<String, GenericType> schemas = new HashMap<String, GenericType>();

    public TranscriptsResponseData() {
        super("oneOf", Boolean.FALSE);
    }

    public TranscriptsResponseData(TranscriptsByIdsResponse o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }
    public TranscriptsResponseData(DocumentResult o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }
    static {
        schemas.put("DocumentResult", new GenericType<DocumentResult>() {
        });
        schemas.put("TranscriptsByIdsResponse", new GenericType<TranscriptsByIdsResponse>() {
        });
        JSON.registerDescendants(TranscriptsResponseData.class, Collections.unmodifiableMap(schemas));
        // Initialize and register the discriminator mappings.
        Map<String, Class<?>> mappings = new HashMap<String, Class<?>>();
        mappings.put("DocumentResult", DocumentResult.class);
        mappings.put("TranscriptsByIdsResponse", TranscriptsByIdsResponse.class);
        mappings.put("documentResult", DocumentResult.class);
        mappings.put("transcriptById", TranscriptsByIdsResponse.class);
        mappings.put("TranscriptsResponseData", TranscriptsResponseData.class);
        JSON.registerDiscriminator(TranscriptsResponseData.class, "transcriptResponseType", mappings);
    }

    @Override
    public Map<String, GenericType> getSchemas() {
        return TranscriptsResponseData.schemas;
    }

    /**
     * Set the instance that matches the oneOf child schema, check
     * the instance parameter is valid against the oneOf child schemas:
     * DocumentResult, TranscriptsByIdsResponse
     *
     * It could be an instance of the 'oneOf' schemas.
     * The oneOf child schemas may themselves be a composed schema (allOf, anyOf, oneOf).
     */
    @Override
    public void setActualInstance(Object instance) {
        // TranscriptsByIdsResponse
        if (JSON.isInstanceOf(TranscriptsByIdsResponse.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        // DocumentResult
        if (JSON.isInstanceOf(DocumentResult.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        throw new RuntimeException("Invalid instance type. Must be DocumentResult, TranscriptsByIdsResponse");
    }

    /**
     * Get the actual instance, which can be the following:
     * DocumentResult, TranscriptsByIdsResponse
     *
     * @return The actual instance (DocumentResult, TranscriptsByIdsResponse)
     */
    @Override
    public Object getActualInstance() {
        return super.getActualInstance();
    }

    /**
     * Get the actual instance of `TranscriptsByIdsResponse`. If the actual instance is not `TranscriptsByIdsResponse`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `TranscriptsByIdsResponse`
     * @throws ClassCastException if the instance is not `TranscriptsByIdsResponse`
     */
    public TranscriptsByIdsResponse getTranscriptsByIdsResponse() throws ClassCastException {
        return (TranscriptsByIdsResponse)super.getActualInstance();
    }

    /**
     * Get the actual instance of `DocumentResult`. If the actual instance is not `DocumentResult`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `DocumentResult`
     * @throws ClassCastException if the instance is not `DocumentResult`
     */
    public DocumentResult getDocumentResult() throws ClassCastException {
        return (DocumentResult)super.getActualInstance();
    }

}

